{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "def mean_absolute_percentage_error(pred, gt):\n",
    "    pred = np.array(pred).flatten()\n",
    "    gt = np.array(gt).flatten()\n",
    "    return (np.absolute(pred - gt) / gt).mean()\n",
    "\n",
    "column_map = {\n",
    "    \"JOUR\": \"DATE\",\n",
    "    \"CODE_STIF_ARRET\": \"STATION_ID\",\n",
    "    \"LIBELLE_ARRET\": \"STATION\",\n",
    "    \"CATEGORIE_TITRE\": \"TICKET_TYPE\",\n",
    "    \"NB_VALD\": \"COUNT\"\n",
    "}\n",
    "\n",
    "stations =  {\n",
    "    \"GARE D'AUSTER\": \"311\",\n",
    "    \"JUSSIEU\": \"383\",\n",
    "    \"CARD.LEMOINE\": \"125\",\n",
    "    \"MAUBERT-MUTUA\": \"525\",\n",
    "    # \"CLUNY LA SORNONNE\": couldn't find\n",
    "    \"ODEON\": \"604\", \n",
    "    \"MABILLON\": \"486\",\n",
    "    \"SEVRES BABELONE\": \"798\",\n",
    "    \"VENEAU\": \"871\", \n",
    "    \"DUROC\": \"246\",\n",
    "    \"SEGUR\": \"791\",\n",
    "    \"LA MOTTE_PICQUET\": \"420\", \n",
    "    \"AV.EMILE ZOLA\": \"43\",\n",
    "    \"CHARLES MICHEL\": \"155\",\n",
    "    \"JAVEL\": \"377\",\n",
    "    \"EGLI.D'AUTEUIL\": \"251\",\n",
    "    \"M.ANGE-AUTEUIL\": \"542\",\n",
    "    \"PORTE D'AUTEUIL\": \"683\", \n",
    "}\n",
    "\n",
    "weekday_map ={\n",
    "    0: 'Monday',\n",
    "    1: 'Tuesday',\n",
    "    2: 'Wednesday',\n",
    "    3: 'Thursday',\n",
    "    4: 'Friday',\n",
    "    5: 'Saturday',\n",
    "    6: 'Sunday'\n",
    "}\n",
    "\n",
    "fares_map = {'NON DEFINI':'normal', \n",
    "    'AMETHYSTE':'elderly', \n",
    "    'NAVIGO':'normal', \n",
    "    'NAVIGO JOUR':'daypass',\n",
    "    'IMAGINE R':'student',\n",
    "    'FGT':'subsidized', \n",
    "    '?':'normal', \n",
    "    'TST':'subsidized',\n",
    "    'AUTRE TITRE':'normal'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path_2015_s1 = \"../data/2015S1_NB_FER.csv\"\n",
    "df_2015_s1 = pd.read_csv(csv_path_2015_s1, sep=';')\n",
    "df_2015_s1['JOUR'] = pd.to_datetime(df_2015_s1['JOUR'], format=\"%d/%m/%Y\").dt.strftime('%Y-%m-%d')\n",
    "\n",
    "csv_path_2015_s2 = \"../data/2015S2_NB_FER.csv\"\n",
    "df_2015_s2 = pd.read_csv(csv_path_2015_s2, sep=';')\n",
    "df_2015_s2['JOUR'] = pd.to_datetime(df_2015_s2['JOUR'], format=\"%d/%m/%Y\").dt.strftime('%Y-%m-%d')\n",
    "\n",
    "df_2015_raw = pd.concat([df_2015_s1, df_2015_s2])\n",
    "df_2015_raw = df_2015_raw[[\"JOUR\", \"CODE_STIF_ARRET\", \"LIBELLE_ARRET\", \"CATEGORIE_TITRE\", \"NB_VALD\"]].rename(columns=column_map)\n",
    "df_2015_raw[\"TICKET_TYPE\"] = df_2015_raw[\"TICKET_TYPE\"].replace(fares_map)\n",
    "df_2015_raw[\"COUNT\"] = df_2015_raw[\"COUNT\"].replace(\"Moins de 5\", 4)\n",
    "df_2015_raw[\"COUNT\"] = df_2015_raw[\"COUNT\"].astype(int)\n",
    "\n",
    "csv_path_2016_s1 = \"../data/2016S1_NB_FER.txt\"\n",
    "df_2016_s1 = pd.read_csv(csv_path_2016_s1, delimiter='\\t')\n",
    "df_2016_s1['JOUR'] = pd.to_datetime(df_2016_s1['JOUR'], format=\"%d/%m/%Y\").dt.strftime('%Y-%m-%d')\n",
    "\n",
    "csv_path_2016_s2 = \"../data/2016S2_NB_FER.txt\"\n",
    "df_2016_s2 = pd.read_csv(csv_path_2016_s2, delimiter='\\t')\n",
    "df_2016_s2['JOUR'] = pd.to_datetime(df_2016_s2['JOUR'], format=\"%d/%m/%Y\").dt.strftime('%Y-%m-%d')\n",
    "\n",
    "df_2016_raw = pd.concat([df_2016_s1, df_2016_s2])\n",
    "df_2016_raw = df_2016_raw[[\"JOUR\", \"CODE_STIF_ARRET\", \"LIBELLE_ARRET\", \"CATEGORIE_TITRE\", \"NB_VALD\"]].rename(columns=column_map)\n",
    "df_2016_raw[\"TICKET_TYPE\"] = df_2016_raw[\"TICKET_TYPE\"].replace(fares_map)\n",
    "df_2016_raw[\"COUNT\"] = df_2016_raw[\"COUNT\"].replace(\"Moins de 5\", 4)\n",
    "df_2016_raw[\"COUNT\"] = df_2016_raw[\"COUNT\"].astype(int)\n",
    "\n",
    "csv_path_2017_s1 = \"../data/2017_S1_NB_FER.txt\"\n",
    "df_2017_s1 = pd.read_csv(csv_path_2017_s1, delimiter='\\t')\n",
    "df_2017_s1['JOUR'] = pd.to_datetime(df_2017_s1['JOUR'], format=\"%d/%m/%Y\").dt.strftime('%Y-%m-%d')\n",
    "\n",
    "csv_path_2017_s2 = \"../data/2017_S2_NB_FER.txt\"\n",
    "df_2017_s2 = pd.read_csv(csv_path_2017_s2, delimiter='\\t')\n",
    "df_2017_s2['JOUR'] = pd.to_datetime(df_2017_s2['JOUR'], format=\"%d/%m/%Y\").dt.strftime('%Y-%m-%d')\n",
    "\n",
    "df_2017_raw = pd.concat([df_2017_s1, df_2017_s2])\n",
    "df_2017_raw = df_2017_raw[[\"JOUR\", \"CODE_STIF_ARRET\", \"LIBELLE_ARRET\", \"CATEGORIE_TITRE\", \"NB_VALD\"]].rename(columns=column_map)\n",
    "df_2017_raw[\"TICKET_TYPE\"] = df_2017_raw[\"TICKET_TYPE\"].replace(fares_map)\n",
    "df_2017_raw[\"COUNT\"] = df_2017_raw[\"COUNT\"].replace(\"Moins de 5\", 4)\n",
    "df_2017_raw[\"COUNT\"] = df_2017_raw[\"COUNT\"].astype(int)\n",
    "\n",
    "\n",
    "csv_path_2018_s1 = \"../data/2018_S1_NB_FER.txt\"\n",
    "df_2018_s1 = pd.read_csv(csv_path_2018_s1, delimiter='\\t')\n",
    "df_2018_s1['JOUR'] = pd.to_datetime(df_2018_s1['JOUR'], format=\"%d/%m/%Y\").dt.strftime('%Y-%m-%d')\n",
    "\n",
    "csv_path_2018_s2 = \"../data/2018_S2_NB_FER.txt\"\n",
    "df_2018_s2 = pd.read_csv(csv_path_2018_s2, delimiter='\\t')\n",
    "df_2018_s2['JOUR'] = pd.to_datetime(df_2018_s2['JOUR'], format=\"%d/%m/%Y\").dt.strftime('%Y-%m-%d')\n",
    "\n",
    "df_2018_raw = pd.concat([df_2018_s1, df_2018_s2])\n",
    "df_2018_raw = df_2018_raw[[\"JOUR\", \"CODE_STIF_ARRET\", \"LIBELLE_ARRET\", \"CATEGORIE_TITRE\", \"NB_VALD\"]].rename(columns=column_map)\n",
    "df_2018_raw[\"TICKET_TYPE\"] = df_2018_raw[\"TICKET_TYPE\"].replace(fares_map)\n",
    "df_2018_raw[\"COUNT\"] = df_2018_raw[\"COUNT\"].replace(\"Moins de 5\", 4)\n",
    "df_2018_raw[\"COUNT\"] = df_2018_raw[\"COUNT\"].astype(int)\n",
    "\n",
    "csv_path_2019_s1 = \"../data/2019_S1_NB_FER.txt\"\n",
    "df_2019_s1 = pd.read_csv(csv_path_2019_s1, delimiter='\\t')\n",
    "df_2019_s1['JOUR'] = pd.to_datetime(df_2019_s1['JOUR'], format=\"%d/%m/%Y\").dt.strftime('%Y-%m-%d')\n",
    "\n",
    "csv_path_2019_s2 = \"../data/2019_S2_NB_FER.txt\"\n",
    "df_2019_s2 = pd.read_csv(csv_path_2019_s2, delimiter='\\t')\n",
    "df_2019_s2['JOUR'] = pd.to_datetime(df_2019_s2['JOUR'], format=\"%d/%m/%Y\").dt.strftime('%Y-%m-%d')\n",
    "\n",
    "df_2019_raw = pd.concat([df_2019_s1, df_2019_s2])\n",
    "df_2019_raw = df_2019_raw[[\"JOUR\", \"CODE_STIF_ARRET\", \"LIBELLE_ARRET\", \"CATEGORIE_TITRE\", \"NB_VALD\"]].rename(columns=column_map)\n",
    "df_2019_raw[\"TICKET_TYPE\"] = df_2019_raw[\"TICKET_TYPE\"].replace(fares_map)\n",
    "df_2019_raw[\"COUNT\"] = df_2019_raw[\"COUNT\"].replace(\"Moins de 5\", 4)\n",
    "df_2019_raw[\"COUNT\"] = df_2019_raw[\"COUNT\"].astype(int)\n",
    "\n",
    "csv_path_2022_s1 = '../data/2022_S1_NB_FER.txt'\n",
    "df_2022_s1 = pd.read_csv(csv_path_2022_s1, delimiter='\\t')\n",
    "df_2022_s1['JOUR'] = pd.to_datetime(df_2022_s1['JOUR'], format=\"%d/%m/%Y\").dt.strftime('%Y-%m-%d')\n",
    "\n",
    "csv_path_2022_s2 = '../data/2022_S2_NB_FER.txt'\n",
    "df_2022_s2 = pd.read_csv(csv_path_2022_s2, delimiter=';')\n",
    "df_2022_s2['JOUR'] = pd.to_datetime(df_2022_s2['JOUR'], format=\"%d/%m/%Y\").dt.strftime('%Y-%m-%d')\n",
    "\n",
    "df_2022_raw = pd.concat([df_2022_s1, df_2022_s2])\n",
    "df_2022_raw = df_2022_raw[[\"JOUR\", \"CODE_STIF_ARRET\", \"LIBELLE_ARRET\", \"CATEGORIE_TITRE\", \"NB_VALD\"]].rename(columns=column_map)\n",
    "df_2022_raw[\"TICKET_TYPE\"] = df_2022_raw[\"TICKET_TYPE\"].replace(fares_map)\n",
    "df_2022_raw[\"COUNT\"] = df_2022_raw[\"COUNT\"].replace(\"Moins de 5\", 4)\n",
    "df_2022_raw[\"COUNT\"] = df_2022_raw[\"COUNT\"].astype(int)\n",
    "\n",
    "csv_path_2023_s1 = '../data/validations-reseau-ferre-nombre-validations-par-jour-1er-semestre.csv'\n",
    "df_2023_s1 = pd.read_csv(csv_path_2023_s1, delimiter=';')\n",
    "df_2023_s1['JOUR'] = pd.to_datetime(df_2023_s1['JOUR'])\n",
    "\n",
    "df_2023_raw = pd.concat([df_2023_s1])\n",
    "df_2023_raw = df_2023_raw[[\"JOUR\", \"CODE_STIF_ARRET\", \"LIBELLE_ARRET\", \"CATEGORIE_TITRE\", \"NB_VALD\"]].rename(columns=column_map)\n",
    "df_2023_raw[\"TICKET_TYPE\"] = df_2023_raw[\"TICKET_TYPE\"].replace(fares_map)\n",
    "df_2023_raw[\"COUNT\"] = df_2023_raw[\"COUNT\"].replace(\"Moins de 5\", 4)\n",
    "df_2023_raw[\"COUNT\"] = df_2023_raw[\"COUNT\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station: GARE D'AUSTER\n",
      "Weekday Average Demands\n",
      " Monday  Tuesday  Wednesday  Thursday  Friday  Saturday  Sunday\n",
      "  21999    22671      22710     21878   21373     14382   13698\n",
      "\n",
      "2024 Prediction (start from Sundat)\n",
      "French Open\n",
      "[20767 20462 20415 20947 22010 20612 19078 21610 20937 21262 20537 21307\n",
      " 21514 19715 22335]\n",
      "\n",
      "Paris Olympic\n",
      "[100555  24555  28016  25968  21896  22018  50889 122507  23219  24755\n",
      "  27660  22232  21395  61466 143361]\n",
      "============================================================\n",
      "\n",
      "Station: JUSSIEU\n",
      "Weekday Average Demands\n",
      " Monday  Tuesday  Wednesday  Thursday  Friday  Saturday  Sunday\n",
      "  11125    11854      12411     11908   11921      6229    3777\n",
      "\n",
      "2024 Prediction (start from Sundat)\n",
      "French Open\n",
      "[ 3761 11372 11656 11650  9751  8344  5322  7779 11220 11260 11308 11450\n",
      " 10670  5701  8453]\n",
      "\n",
      "Paris Olympic\n",
      "[  3778  11255  11932  13521  21210  37476   9373 104738  11144  12562\n",
      "  14744  12327  15046   7294 141609]\n",
      "============================================================\n",
      "\n",
      "Station: CARD.LEMOINE\n",
      "Weekday Average Demands\n",
      " Monday  Tuesday  Wednesday  Thursday  Friday  Saturday  Sunday\n",
      "   3576     4051       4296      4171    4444      3653    2105\n",
      "\n",
      "2024 Prediction (start from Sundat)\n",
      "French Open\n",
      "[2383 4408 4612 4606 4077 3878 3043 3514 4317 4514 4436 4527 4533 3484\n",
      " 3371]\n",
      "\n",
      "Paris Olympic\n",
      "[ 2979  8184  5900  4828  4221  6160  6078 24560  7231  5310  4404  4894\n",
      "  4486  3839 20233]\n",
      "============================================================\n",
      "\n",
      "Station: MAUBERT-MUTUA\n",
      "Weekday Average Demands\n",
      " Monday  Tuesday  Wednesday  Thursday  Friday  Saturday  Sunday\n",
      "   3839     4524       4732      4599    4708      4171    2579\n",
      "\n",
      "2024 Prediction (start from Sundat)\n",
      "French Open\n",
      "[2825 4781 4958 4976 5046 4766 3535 3881 5464 4888 4973 5052 4640 3790\n",
      " 3963]\n",
      "\n",
      "Paris Olympic\n",
      "[ 3137  9342  5515  5031  5633  4725  6480 18229 20216  5221  5024  5661\n",
      "  4731  4999 20262]\n",
      "============================================================\n",
      "\n",
      "Station: ODEON\n",
      "Weekday Average Demands\n",
      " Monday  Tuesday  Wednesday  Thursday  Friday  Saturday  Sunday\n",
      "  11117    12508      12835     12336   13688     12335    5811\n",
      "\n",
      "2024 Prediction (start from Sundat)\n",
      "French Open\n",
      "[ 8019 12696 12745 12738 12589 12811 10948 10970 13210 12803 12829 12701\n",
      " 12694 12155 10857]\n",
      "\n",
      "Paris Olympic\n",
      "[ 25786  16456  12614  12852  12459  15025  16048 114862  20499  12673\n",
      "  12835  12593  15406  12397 110137]\n",
      "============================================================\n",
      "\n",
      "Station: MABILLON\n",
      "Weekday Average Demands\n",
      " Monday  Tuesday  Wednesday  Thursday  Friday  Saturday  Sunday\n",
      "   3645     4080       4443      4281    4322      4165    2468\n",
      "\n",
      "2024 Prediction (start from Sundat)\n",
      "French Open\n",
      "[2841 4633 5060 5113 4598 5627 3467 4019 4845 5250 5344 5246 5002 3845\n",
      " 4280]\n",
      "\n",
      "Paris Olympic\n",
      "[ 3810 10021  9684  6848  4839 13703  6950 25675 13051 12068  8793  9460\n",
      "  6869  4750 34143]\n",
      "============================================================\n",
      "\n",
      "Station: SEVRES BABELONE\n",
      "Weekday Average Demands\n",
      " Monday  Tuesday  Wednesday  Thursday  Friday  Saturday  Sunday\n",
      "  10982    12424      12664     12451   12270     10791    6153\n",
      "\n",
      "2024 Prediction (start from Sundat)\n",
      "French Open\n",
      "[ 7939 11891 11982 11886 11425 11452  9098 10144 11778 11885 11846 11939\n",
      " 11984 10899 10096]\n",
      "\n",
      "Paris Olympic\n",
      "[18496 12773 12798 13801 14463 13568 17115 67787 12355 12980 13922 12952\n",
      " 12428 10816 66314]\n",
      "============================================================\n",
      "\n",
      "Station: VENEAU\n",
      "Weekday Average Demands\n",
      " Monday  Tuesday  Wednesday  Thursday  Friday  Saturday  Sunday\n",
      "   2522     2792       2857      2766    2672      1729     969\n",
      "\n",
      "2024 Prediction (start from Sundat)\n",
      "French Open\n",
      "[1194 3095 3130 3175 2846 2092 1539 2592 3222 3199 3157 3049 2777 1891\n",
      " 2274]\n",
      "\n",
      "Paris Olympic\n",
      "[ 2212  5621  3766  3699  2821  5669  2226 65692  7147  4204  3607  3455\n",
      "  2770  2090 42814]\n",
      "============================================================\n",
      "\n",
      "Station: DUROC\n",
      "Weekday Average Demands\n",
      " Monday  Tuesday  Wednesday  Thursday  Friday  Saturday  Sunday\n",
      "   8665     9241       9574      9325    8970      5125    3322\n",
      "\n",
      "2024 Prediction (start from Sundat)\n",
      "French Open\n",
      "[ 4064 10339 10605 10724  9993  6164  4203 10423 10665 10683 10677 10511\n",
      "  9392  4580  9799]\n",
      "\n",
      "Paris Olympic\n",
      "[  7268  16365  14034  12862  10464  29869   9074 364723  19656  14598\n",
      "  12599  12916   9442   6504 303998]\n",
      "============================================================\n",
      "\n",
      "Station: SEGUR\n",
      "Weekday Average Demands\n",
      " Monday  Tuesday  Wednesday  Thursday  Friday  Saturday  Sunday\n",
      "   3723     4121       4300      4011    4109      2743    1559\n",
      "\n",
      "2024 Prediction (start from Sundat)\n",
      "French Open\n",
      "[2303 4935 4938 4946 5550 3622 2368 4825 5351 5070 5145 5012 4103 2616\n",
      " 4508]\n",
      "\n",
      "Paris Olympic\n",
      "[ 10012  13117   7977   6610  18070   5483   3963 164464  20672   9324\n",
      "   8253   9958   4109   2883 134376]\n",
      "============================================================\n",
      "\n",
      "Station: LA MOTTE_PICQUET\n",
      "Weekday Average Demands\n",
      " Monday  Tuesday  Wednesday  Thursday  Friday  Saturday  Sunday\n",
      "  16246    17618      18658     18214   18342     14446    9348\n",
      "\n",
      "2024 Prediction (start from Sundat)\n",
      "French Open\n",
      "[ 9913 20321 21206 20622 19223 17547 11553 18916 20776 21445 21281 21515\n",
      " 18818 13435 18913]\n",
      "\n",
      "Paris Olympic\n",
      "[ 10161  40582  35015  23580  19544  19162  28240 242518  46320  37410\n",
      "  27437  32458  18636  16130 242372]\n",
      "============================================================\n",
      "\n",
      "Station: AV.EMILE ZOLA\n",
      "Weekday Average Demands\n",
      " Monday  Tuesday  Wednesday  Thursday  Friday  Saturday  Sunday\n",
      "   3511     3877       4071      3807    3986      2942    1742\n",
      "\n",
      "2024 Prediction (start from Sundat)\n",
      "French Open\n",
      "[2222 4669 4823 4761 4885 3737 2334 4604 4769 4814 4790 4740 3933 2691\n",
      " 4145]\n",
      "\n",
      "Paris Olympic\n",
      "[  4891  12604   9372   6855  11074   4356   5933 113696  14243   9268\n",
      "   7094   9251   4002   3451  80666]\n",
      "============================================================\n",
      "\n",
      "Station: CHARLES MICHEL\n",
      "Weekday Average Demands\n",
      " Monday  Tuesday  Wednesday  Thursday  Friday  Saturday  Sunday\n",
      "   9391    10126      10621     10208   10886      9273    5944\n",
      "\n",
      "2024 Prediction (start from Sundat)\n",
      "French Open\n",
      "[ 6437 10754 11278 11277 11308  9831  7585  9235 10920 11212 11228 11368\n",
      " 10616  8878  8793]\n",
      "\n",
      "Paris Olympic\n",
      "[ 6917 14101 13246 11585 13030 13320 16589 49327 15318 12899 11446 13346\n",
      " 11045  9673 38456]\n",
      "============================================================\n",
      "\n",
      "Station: JAVEL\n",
      "Weekday Average Demands\n",
      " Monday  Tuesday  Wednesday  Thursday  Friday  Saturday  Sunday\n",
      "   5731     6267       6438      6145    6108      3886    2402\n",
      "\n",
      "2024 Prediction (start from Sundat)\n",
      "French Open\n",
      "[5746 6856 6901 6567 5023 4327 6215 6898 6902 6922 6807 5075 3095]\n",
      "\n",
      "Paris Olympic\n",
      "[113245  10989   7794   6499  11022  18472   6704   7779   7234   8484\n",
      "   8012  12547   7162]\n",
      "============================================================\n",
      "\n",
      "Station: EGLI.D'AUTEUIL\n",
      "Weekday Average Demands\n",
      " Monday  Tuesday  Wednesday  Thursday  Friday  Saturday  Sunday\n",
      "    410      430        454       436     427       278     195\n",
      "\n",
      "2024 Prediction (start from Sundat)\n",
      "French Open\n",
      "[249 550 545 541 522 377 237 473 543 540 566 522 447 282 455]\n",
      "\n",
      "Paris Olympic\n",
      "[ 551 1548 1162  850  839  566  421 9631 1437 1099 1111  839  449  279\n",
      " 8448]\n",
      "============================================================\n",
      "\n",
      "Station: M.ANGE-AUTEUIL\n",
      "Weekday Average Demands\n",
      " Monday  Tuesday  Wednesday  Thursday  Friday  Saturday  Sunday\n",
      "   5241     5670       5970      5806    5747      3607    2151\n",
      "\n",
      "2024 Prediction (start from Sundat)\n",
      "French Open\n",
      "[3947 7069 7305 7503 8538 5379 3633 7779 7633 7545 5540 7248 5558 3851\n",
      " 6203]\n",
      "\n",
      "Paris Olympic\n",
      "[ 37855  20421  16895  15342  36414   6308   3611 352756  31234  20432\n",
      "   6707  14333   5894   3999 183890]\n",
      "============================================================\n",
      "\n",
      "Station: PORTE D'AUTEUIL\n",
      "Weekday Average Demands\n",
      " Monday  Tuesday  Wednesday  Thursday  Friday  Saturday  Sunday\n",
      "   1018     1065       1163      1100    1095      1082     882\n",
      "\n",
      "2024 Prediction (start from Sundat)\n",
      "French Open\n",
      "[2738 2699 2962 2858 2696 3164 2452 2674 2714 2763 2799 2617 2748 2222\n",
      " 1614]\n",
      "\n",
      "Paris Olympic\n",
      "[93872 67108 81516 59981 56234 94175 42383 87569 68293 65522 55957 50911\n",
      " 60508 29679 15346]\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for station, id in stations.items():\n",
    "    print(\"Station:\", station)\n",
    "    df_2015 = df_2015_raw[df_2015_raw[\"STATION_ID\"] == id]\n",
    "    df_2015 = df_2015.groupby([\"DATE\", \"STATION_ID\", \"STATION\"]).sum().reset_index()\n",
    "    df_2015 = df_2015[[\"DATE\", \"STATION_ID\", \"COUNT\"]]\n",
    "    df_2015[\"DAY_OF_WEEK\"] = pd.to_datetime(df_2015[\"DATE\"]).dt.day_of_week\n",
    "    df_2015[\"DAY_OF_WEEK\"] = df_2015[\"DAY_OF_WEEK\"].replace(weekday_map)\n",
    "\n",
    "    df_2016 = df_2016_raw[df_2016_raw[\"STATION_ID\"] == id]\n",
    "    df_2016 = df_2016.groupby([\"DATE\", \"STATION_ID\", \"STATION\"]).sum().reset_index()\n",
    "    df_2016 = df_2016[[\"DATE\", \"STATION_ID\", \"COUNT\"]]\n",
    "    df_2016[\"DAY_OF_WEEK\"] = pd.to_datetime(df_2016[\"DATE\"]).dt.day_of_week\n",
    "    df_2016[\"DAY_OF_WEEK\"] = df_2016[\"DAY_OF_WEEK\"].replace(weekday_map)\n",
    "\n",
    "    df_2017 = df_2017_raw[df_2017_raw[\"STATION_ID\"] == id]\n",
    "    df_2017 = df_2017.groupby([\"DATE\", \"STATION_ID\"]).sum().reset_index()\n",
    "    df_2017 = df_2017[[\"DATE\", \"STATION_ID\", \"COUNT\"]]\n",
    "    df_2017[\"DAY_OF_WEEK\"] = pd.to_datetime(df_2017[\"DATE\"]).dt.day_of_week\n",
    "    df_2017[\"DAY_OF_WEEK\"] = df_2017[\"DAY_OF_WEEK\"].replace(weekday_map)\n",
    "\n",
    "    df_2018 = df_2018_raw[df_2018_raw[\"STATION_ID\"] == id]\n",
    "    df_2018 = df_2018.groupby([\"DATE\", \"STATION_ID\"]).sum().reset_index()\n",
    "    df_2018 = df_2018[[\"DATE\", \"STATION_ID\", \"COUNT\"]]\n",
    "    df_2018[\"DAY_OF_WEEK\"] = pd.to_datetime(df_2018[\"DATE\"]).dt.day_of_week\n",
    "    df_2018[\"DAY_OF_WEEK\"] = df_2018[\"DAY_OF_WEEK\"].replace(weekday_map)\n",
    "\n",
    "    df_2019 = df_2019_raw[df_2019_raw[\"STATION_ID\"] == id]\n",
    "    df_2019 = df_2019.groupby([\"DATE\", \"STATION_ID\"]).sum().reset_index()\n",
    "    df_2019 = df_2019[[\"DATE\", \"STATION_ID\", \"COUNT\"]]\n",
    "    df_2019[\"DAY_OF_WEEK\"] = pd.to_datetime(df_2019[\"DATE\"]).dt.day_of_week\n",
    "    df_2019[\"DAY_OF_WEEK\"] = df_2019[\"DAY_OF_WEEK\"].replace(weekday_map)\n",
    "\n",
    "    df_2022 = df_2022_raw[df_2022_raw[\"STATION_ID\"] == id]\n",
    "    df_2022 = df_2022.groupby([\"DATE\", \"STATION_ID\"]).sum().reset_index()\n",
    "    df_2022 = df_2022[[\"DATE\", \"STATION_ID\", \"COUNT\"]]\n",
    "    df_2022[\"DAY_OF_WEEK\"] = pd.to_datetime(df_2022[\"DATE\"]).dt.day_of_week\n",
    "    df_2022[\"DAY_OF_WEEK\"] = df_2022[\"DAY_OF_WEEK\"].replace(weekday_map)\n",
    "\n",
    "    df_2023 = df_2023_raw[df_2023_raw[\"STATION_ID\"] == id]\n",
    "    df_2023 = df_2023.groupby([\"DATE\", \"STATION_ID\"]).sum().reset_index()\n",
    "    df_2023 = df_2023[[\"DATE\", \"STATION_ID\", \"COUNT\"]]\n",
    "    df_2023[\"DAY_OF_WEEK\"] = pd.to_datetime(df_2023[\"DATE\"]).dt.day_of_week\n",
    "    df_2023[\"DAY_OF_WEEK\"] = df_2023[\"DAY_OF_WEEK\"].replace(weekday_map)\n",
    "\n",
    "    french_open_data = {\n",
    "        2015: df_2015,\n",
    "        2016: df_2016,\n",
    "        2017: df_2017,\n",
    "        2018: df_2018,\n",
    "        2019: df_2019,\n",
    "        2022: df_2022,\n",
    "        2023: df_2023\n",
    "    }\n",
    "\n",
    "    # The start date is the day when Fans Day starts since the demand already reflects the impact of French Open.\n",
    "    # Moreover, there will be more training data so it will be more accurate in my opinion.\n",
    "    start_dates = [\"2015-05-17\", \"2016-05-15\", \"2017-05-21\", \"2018-05-20\", \"2019-05-19\", \"2022-05-15\", \"2023-05-22\"]\n",
    "    end_dates = [\"2015-06-07\", \"2016-06-05\", \"2017-06-11\", \"2018-06-10\", \"2019-06-09\", \"2022-06-05\", \"2023-06-12\"] # for 2023, to make the length same I use 12th for end date\n",
    "    counts = pd.concat([\n",
    "        df_2015[(df_2015[\"DATE\"] >= start_dates[0]) & (df_2015[\"DATE\"] <= end_dates[0])][\"COUNT\"], \n",
    "        df_2016[(df_2016[\"DATE\"] >= start_dates[1]) & (df_2016[\"DATE\"] <= end_dates[1])][\"COUNT\"], \n",
    "        df_2017[(df_2017[\"DATE\"] >= start_dates[2]) & (df_2017[\"DATE\"] <= end_dates[2])][\"COUNT\"], \n",
    "        df_2018[(df_2018[\"DATE\"] >= start_dates[3]) & (df_2018[\"DATE\"] <= end_dates[3])][\"COUNT\"], \n",
    "        df_2019[(df_2019[\"DATE\"] >= start_dates[4]) & (df_2019[\"DATE\"] <= end_dates[4])][\"COUNT\"], \n",
    "        df_2022[(df_2022[\"DATE\"] >= start_dates[5]) & (df_2022[\"DATE\"] <= end_dates[5])][\"COUNT\"], \n",
    "        df_2023[(df_2023[\"DATE\"] >= start_dates[6]) & (df_2023[\"DATE\"] <= end_dates[6])][\"COUNT\"]]).values\n",
    "    \n",
    "    # I try to scale the data to [0, 1] to make the model more stable and easier to learn. The value will be scaled back to the original scale later.\n",
    "    fo_scaler = MinMaxScaler()\n",
    "    fo_scaler.fit(counts.reshape(-1, 1))\n",
    "    for key, df in french_open_data.items():\n",
    "        df[\"COUNT_SCALE\"] = fo_scaler.transform(df[\"COUNT\"].values.reshape(-1, 1))\n",
    "\n",
    "    # ensure no data lost, which each data should be 22 days\n",
    "    # for idx, (year, data) in enumerate(french_open_data.items()):\n",
    "    #     print(\"{}: {} days\".format(year, len(data[(data[\"DATE\"] >= start_dates[idx]) & (data[\"DATE\"] <= end_dates[idx])])))\n",
    "\n",
    "\n",
    "    # The first two years are marked as input and the third will be the output.\n",
    "    # For example, day 1 of French Open in 2015 and 2016 will be the input, day 1 in 2017 will be the output.\n",
    "    # The real french open days are 15 days\n",
    "    french_open_days = 15\n",
    "    if station == \"JAVEL\": # Javel station lost two data in 2023, the 7 and 8 days\n",
    "        french_open_days = 13\n",
    "        \n",
    "    years = list(french_open_data.keys())\n",
    "    fo_dataset = [] # fo represents French Open\n",
    "    for idx in range(2, len(years)):\n",
    "        first = french_open_data[years[idx - 2]]\n",
    "        first = first[(first[\"DATE\"] >= start_dates[idx - 2]) & (first[\"DATE\"] <= end_dates[idx - 2])]\n",
    "\n",
    "        second = french_open_data[years[idx - 1]]\n",
    "        second = second[(second[\"DATE\"] >= start_dates[idx - 1]) & (second[\"DATE\"] <= end_dates[idx - 1])]\n",
    "\n",
    "        if (years[idx] == 2023 and station == \"JAVEL\"):\n",
    "            first = first.drop(index=first.iloc[6:8, :].index.to_list())\n",
    "            second = second.drop(index=second.iloc[6:8, :].index.to_list())\n",
    "\n",
    "        third = french_open_data[years[idx]]\n",
    "        third = third[(third[\"DATE\"] >= start_dates[idx]) & (third[\"DATE\"] <= end_dates[idx])]\n",
    "\n",
    "        for i in range(french_open_days):\n",
    "            day_in_first = first.iloc[i][\"COUNT_SCALE\"]\n",
    "            day_in_second = second.iloc[i][\"COUNT_SCALE\"]\n",
    "            day_in_third = third.iloc[i][\"COUNT_SCALE\"]\n",
    "            fo_dataset.append([day_in_first, day_in_second, day_in_third])\n",
    "\n",
    "    fo_dataset = np.array(fo_dataset)\n",
    "    fo_dataset_x, fo_dataset_y = fo_dataset[:, :-1], fo_dataset[:, [-1]]\n",
    "    # 90% of dataset will be used for training and 10% for validation\n",
    "    train_x, validate_x, train_y, validate_y = train_test_split(fo_dataset_x, fo_dataset_y, test_size=0.1, shuffle=True)\n",
    "    # print(\"after split:\", train_x.shape, train_y.shape, validate_x.shape, validate_y.shape)\n",
    "\n",
    "    # Train the model\n",
    "    fo_model = SVR(C=10, cache_size=200, coef0=0.0, degree=3, epsilon=0.05, gamma=0.5,\n",
    "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "    fo_model.fit(train_x, train_y[:, 0])\n",
    "\n",
    "    # Now create the testing data, which using 2022 and 2023 data to predict 2024.\n",
    "    # The process are the same.\n",
    "    test_data = []\n",
    "    first = french_open_data[2022]\n",
    "    first = first[(first[\"DATE\"] >= \"2022-05-22\") & (first[\"DATE\"] <= \"2022-06-05\")]\n",
    "    if station == \"JAVEL\":\n",
    "        first = first.drop(index=first.iloc[7:9, :].index.to_list())\n",
    "\n",
    "    second = french_open_data[2023]\n",
    "    second = second[(second[\"DATE\"] >= \"2023-05-29\") & (second[\"DATE\"] <= \"2023-06-12\")]\n",
    "    for i in range(french_open_days):\n",
    "        first_day = first.iloc[i][\"COUNT_SCALE\"]\n",
    "        second_day = second.iloc[i][\"COUNT_SCALE\"]\n",
    "        test_data.append([first_day, second_day])\n",
    "\n",
    "    test_data = np.array(test_data)\n",
    "    test_pred = fo_model.predict(test_data).reshape(-1,1)\n",
    "\n",
    "    # y_test_pred is the prediction of 2024 French Open, which has 15 days\n",
    "    y_test_pred = fo_scaler.inverse_transform(test_pred) # scale back to the original scale\n",
    "\n",
    "\n",
    "    # calculate the non-french open days average passenger demand in normal days based on 2022, 2023 data\n",
    "    df_2022_day = df_2022[(df_2022[\"DATE\"] <= \"2022-05-15\") | (df_2022[\"DATE\"] >= \"2022-06-05\")]\n",
    "    df_2023_day = df_2023[(df_2023[\"DATE\"] <= \"2023-05-22\") | (df_2023[\"DATE\"] >= \"2023-06-12\")]\n",
    "    df_day = pd.concat([df_2022_day, df_2023_day])[[\"DAY_OF_WEEK\", \"COUNT\"]]\n",
    "    df_day_mean = df_day.groupby([\"DAY_OF_WEEK\"]).mean().astype(int).reset_index()\n",
    "    weekday_demand = df_day_mean[\"COUNT\"].values\n",
    "    df_day_of_week = pd.DataFrame({\n",
    "        \"Monday\": [weekday_demand[1]],\n",
    "        \"Tuesday\": [weekday_demand[5]],\n",
    "        \"Wednesday\": [weekday_demand[6]],\n",
    "        \"Thursday\": [weekday_demand[4]],\n",
    "        \"Friday\": [weekday_demand[0]],\n",
    "        \"Saturday\": [weekday_demand[2]],\n",
    "        \"Sunday\": [weekday_demand[3]]\n",
    "    })\n",
    "    print(\"Weekday Average Demands\")\n",
    "    print(df_day_of_week.to_string(index=False))\n",
    "    \n",
    "    day_of_week = np.array([weekday_demand[3], weekday_demand[1], weekday_demand[5], weekday_demand[6], weekday_demand[4], weekday_demand[0], weekday_demand[2],\n",
    "                   weekday_demand[3], weekday_demand[1], weekday_demand[5], weekday_demand[6], weekday_demand[4], weekday_demand[0], weekday_demand[2],\n",
    "                   weekday_demand[3]])\n",
    "    if station == \"JAVEL\":\n",
    "        day_of_week = np.array([weekday_demand[3], weekday_demand[1], weekday_demand[5], weekday_demand[6], weekday_demand[4], weekday_demand[0], \n",
    "                   weekday_demand[1], weekday_demand[5], weekday_demand[6], weekday_demand[4], weekday_demand[0], weekday_demand[2],\n",
    "                   weekday_demand[3]])\n",
    "        \n",
    "\n",
    "    # scale to Olympic prediction\n",
    "    fo_2024_attendence_from_news = 630000\n",
    "    op_2024_attendence_from_news = 15000000\n",
    "    ratio = op_2024_attendence_from_news / fo_2024_attendence_from_news\n",
    "\n",
    "    fo_2024_expected_attendence = y_test_pred.reshape(-1).astype(int)\n",
    "    print(\"\\n2024 Prediction (start from Sundat)\")\n",
    "    print(\"French Open\")\n",
    "    print(fo_2024_expected_attendence.astype(int))\n",
    "    # Instead of just scale from the ratio, I add the amount based on French Open demand increased percentage compared to normal days.\n",
    "    # I dont think its reasonable to scale the data directly since some station doesn't really change much during French Open. Scaling the increase percentage probably make more sense.\n",
    "    # since depends on the data, some station will lose demand, which doesnt really make sense on while Olympic, so I set all the increasement to be positive\n",
    "    increasements =(fo_2024_expected_attendence - day_of_week) / day_of_week * ratio\n",
    "    op_2024_expected_attendence =  np.abs((fo_2024_expected_attendence - day_of_week) * increasements) + day_of_week\n",
    "    print(\"\\nParis Olympic\")\n",
    "    print(op_2024_expected_attendence.astype(int))\n",
    "\n",
    "    # print(day_of_week * increasement)\n",
    "    print(\"============================================================\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Algo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
